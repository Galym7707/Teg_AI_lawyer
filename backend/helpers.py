# -*- coding: utf-8 -*-
import os
import json
import re
import logging
from typing import List, Dict, Tuple

from rank_bm25 import BM25Okapi

import google.generativeai as genai

log = logging.getLogger(__name__)

# ---------- –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞ ----------
def _read_jsonl(path: str) -> List[Dict]:
    items = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            items.append(json.loads(line))
    return items

def load_normalized(path: str) -> List[Dict]:
    if not os.path.isabs(path):
        base = os.path.dirname(os.path.abspath(__file__))
        path = os.path.join(base, path)
    if not os.path.exists(path):
        log.warning("normalized.jsonl –Ω–µ –Ω–∞–π–¥–µ–Ω: %s", path)
        return []
    items = _read_jsonl(path)
    return items

# fallback ‚Äî –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è –±–∞–∑–∞, –µ—Å–ª–∏ normalized –Ω–µ—Ç
def load_laws(path: str) -> List[Dict]:
    if not os.path.isabs(path):
        base = os.path.dirname(os.path.abspath(__file__))
        path = os.path.join(base, path)
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

# ---------- –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è ----------
WORD_RE = re.compile(r"[a-zA-Z–∞-—è–ê-–Ø—ë–Å0-9\-]+")

def _tok(s: str) -> List[str]:
    return WORD_RE.findall(s.lower())

# ---------- –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è BM25 ----------
class LawIndex:
    def __init__(self, docs: List[Dict]):
        self.docs = docs
        # –∏–Ω–¥–µ–∫—Å —Å—Ç—Ä–æ–∏–º –ø–æ plain_summary –µ—Å–ª–∏ –µ—Å—Ç—å, –∏–Ω–∞—á–µ –ø–æ plain_text
        corpus = []
        for d in docs:
            text = d.get("plain_summary") or d.get("plain_text") or ""
            corpus.append(_tok(text))
        self.bm25 = BM25Okapi(corpus)

    def search(self, query: str, top_k: int = 5) -> List[Tuple[Dict, float]]:
        q = _tok(query or "")
        if not q:
            return []
        scores = self.bm25.get_scores(q)
        idx_scores = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)[:top_k]
        return [(self.docs[i], float(s)) for i, s in idx_scores if s > 0.0]

# ---------- –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ (fallback) ----------
def build_html_answer(question: str, hits: List[Tuple[Dict, float]], intent: Dict) -> str:
    parts = [f"<h3>–Æ—Ä–∏–¥–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞</h3>"]
    if hits:
        parts.append("<p>–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –Ω–æ—Ä–º—ã –ø–æ –≤–∞—à–µ–º—É –≤–æ–ø—Ä–æ—Å—É:</p>")
        parts.append("<ul>")
        for rec, score in hits:
            title = rec.get("article_title") or rec.get("law_title") or "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"
            src = rec.get("source") or ""
            parts.append(f"<li><strong>{title}</strong>{' ‚Äî ' + src if src else ''}</li>")
        parts.append("</ul>")
        # –∫—Ä–∞—Ç–∫–∏–µ —Ç–µ–∑–∏—Å—ã (–µ—Å–ª–∏ –µ—Å—Ç—å plain_summary)
        bullets = []
        for rec, _ in hits[:2]:
            summ = (rec.get("plain_summary") or "").strip()
            if summ:
                bullets.append(summ)
        if bullets:
            parts.append("<h3>–ü–æ —Å—É—Ç–∏</h3>")
            for b in bullets:
                parts.append(f"<p>{b}</p>")
    else:
        parts.append("<p>–ü—Ä—è–º—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ. –ù–∏–∂–µ ‚Äî –æ–±—â–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º –ø–æ —Ç–∏–ø–æ–≤–æ–π –ø—Ä–∞–∫—Ç–∏–∫–µ –†–ö.</p>")

    parts.append("<h3>–ß—Ç–æ –¥–µ–ª–∞—Ç—å –ø–æ—à–∞–≥–æ–≤–æ</h3>")
    parts.append("<ul>"
                 "<li>–û–ø–∏—à–∏—Ç–µ —Å–∏—Ç—É–∞—Ü–∏—é (–¥–∞—Ç—ã, —Å—Ç–æ—Ä–æ–Ω—ã, –¥–æ–∫—É–º–µ–Ω—Ç—ã, —Å—Ç–∞—Ç—É—Å—ã).</li>"
                 "<li>–ü–æ–¥–≥–æ—Ç–æ–≤—å—Ç–µ –±–∞–∑–æ–≤—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã (—É–¥–æ—Å—Ç–æ–≤–µ—Ä–µ–Ω–∏–µ –ª–∏—á–Ω–æ—Å—Ç–∏, –¥–æ–≥–æ–≤–æ—Ä—ã, –ø–µ—Ä–µ–ø–∏—Å–∫—É).</li>"
                 "<li>–°–ª–µ–¥—É–π—Ç–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö —Å—Ç–∞—Ç–µ–π, –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –æ—Ñ–æ—Ä–º–∏—Ç–µ –∑–∞—è–≤–ª–µ–Ω–∏–µ/—É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ.</li>"
                 "</ul>")
    return "\n".join(parts)

# ---------- –ü–æ–∏—Å–∫ ----------
def init_index() -> Tuple[List[Dict], LawIndex]:
    norm = load_normalized("laws/normalized.jsonl")
    if norm:
        log.info("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –∫–æ—Ä–ø—É—Å: %d —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤", len(norm))
        return norm, LawIndex(norm)
    # —Ñ–æ–ª–ª–±–µ–∫
    raw = load_laws("laws/kazakh_laws.json")
    # –ø—Ä–∏–≤–µ–¥—ë–º –∫ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É –≤–∏–¥—É
    docs = []
    for x in raw:
        docs.append({
            "law_title": (x.get("title") or "").strip() or "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è",
            "article_title": (x.get("title") or "").strip(),
            "source": x.get("source"),
            "plain_text": (x.get("text") or "").strip()
        })
    log.info("‚ö†Ô∏è normalized.jsonl –Ω–µ—Ç. –†–∞–±–æ—Ç–∞–µ–º –ø–æ —Å—ã—Ä–æ–º—É –∫–æ—Ä–ø—É—Å—É: %d", len(docs))
    return docs, LawIndex(docs)

def search_laws(question: str, docs: List[Dict], index: LawIndex, top_k: int = 5):
    hits = index.search(question, top_k=top_k)
    intent = {"name": "generic"}
    return hits, intent

# ---------- –ò–ò ----------
def _init_llm():
    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        log.warning("GEMINI_API_KEY –Ω–µ –∑–∞–¥–∞–Ω ‚Äî LLM –æ—Ç–∫–ª—é—á—ë–Ω.")
        return None
    genai.configure(api_key=api_key)
    model_name = os.getenv("GEMINI_MODEL", "gemini-1.5-flash")
    model = genai.GenerativeModel(
        model_name=model_name,
        generation_config={"temperature": 0.25, "max_output_tokens": 1600},
        system_instruction=(
            "–¢—ã ‚Äî –ò–ò-—é—Ä–∏—Å—Ç –ø–æ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤—É –†–ö. –û—Ç–≤–µ—á–∞–π —Å—Ç—Ä–æ–≥–æ –≤ –ß–ò–°–¢–û–ú HTML "
            "(<p>, <ul>, <ol>, <li>, <strong>, <em>, <h3>, <a>), –±–µ–∑ Markdown. "
            "–°—Ç—Ä—É–∫—Ç—É—Ä–∞: "
            "<h3>–Æ—Ä–∏–¥–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞</h3> –∫–æ—Ä–æ—Ç–∫–æ –∏ –ø—Ä–µ–¥–º–µ—Ç–Ω–æ; "
            "<h3>–ß—Ç–æ –¥–µ–ª–∞—Ç—å –ø–æ—à–∞–≥–æ–≤–æ</h3> 3‚Äì8 –∫–æ—Ä–æ—Ç–∫–∏—Ö —à–∞–≥–æ–≤; "
            "<h3>–ù–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –æ—Å–Ω–æ–≤–∞–Ω–∏—è</h3> –ø–µ—Ä–µ—á–∏—Å–ª–∏ —Å—Ç–∞—Ç—å–∏/–∞–∫—Ç—ã; "
            "<h3>–®–∞–±–ª–æ–Ω—ã/–¥–æ–∫—É–º–µ–Ω—Ç—ã</h3> —á—Ç–æ –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å. "
            "–ù–µ —Å–æ–≤–µ—Ç—É–π ¬´–æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –∫ —é—Ä–∏—Å—Ç—É¬ª, –¥–∞–π —Å–∞–º –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —à–∞–≥–∏. "
            "–ï—Å–ª–∏ —Ç–æ—á–Ω—ã—Ö –Ω–æ—Ä–º –Ω–µ—Ç ‚Äî –¥–∞–π –æ–±—â–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º (—Ç–∏–ø–æ–≤–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞ –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω–∞)."
        )
    )
    log.info("ü§ñ Gemini –≥–æ—Ç–æ–≤: %s", model_name)
    return model

_MODEL = _init_llm()

def call_llm(question: str, hits: List[Tuple[Dict, float]]) -> str:
    if _MODEL is None:
        return ""
    # –∫–æ–º–ø–∞–∫—Ç–Ω—ã–π HTML-–∫–æ–Ω—Ç–µ–∫—Å—Ç
    ctx_parts = []
    for rec, _ in hits[:3]:
        t = rec.get("plain_summary") or rec.get("plain_text") or ""
        if len(t) > 1200:
            t = t[:1200] + "‚Ä¶"
        art = rec.get("article_title") or rec.get("law_title") or "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"
        src = rec.get("source") or ""
        ctx_parts.append(f"<p><strong>{art}</strong>{(' ‚Äî ' + src) if src else ''}</p><p>{t}</p>")
    ctx_html = "\n".join(ctx_parts) or "<p>–¢–æ—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.</p>"

    prompt = (
        "<h3>–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è</h3>"
        f"<p>{question}</p>"
        "<h3>–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –≤—ã–¥–µ—Ä–∂–∫–∏</h3>"
        f"{ctx_html}"
        "<p>–°–æ–±–µ—Ä–∏ –∏—Ç–æ–≥ —Ç–æ–ª—å–∫–æ –≤ —á–∏—Å—Ç–æ–º HTML.</p>"
    )
    try:
        r = _MODEL.generate_content(prompt)
        txt = (r.text or "").strip()
        # –ù–∞ –≤—Å—è–∫–∏–π ‚Äî –µ—Å–ª–∏ –≤–µ—Ä–Ω—ë—Ç Markdown, –ø–æ–¥—á–∏—â–∞–µ–º
        txt = re.sub(r"\*\*(.+?)\*\*", r"<strong>\1</strong>", txt)
        txt = txt.replace("\n", "<br>")
        return txt
    except Exception as e:
        log.exception("LLM error: %s", e)
        return ""
