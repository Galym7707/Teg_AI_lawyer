# -*- coding: utf-8 -*-
"""
Kaz Legal Bot ‚Äì Flask backend
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –∑–∞–∫–æ–Ω–æ–≤ + HTML-–æ—Ç–≤–µ—Ç—ã + fallback –Ω–∞ Adilet
"""

import os
import re
import json
import time
import logging
from typing import List, Dict, Any, Iterable, Tuple
from difflib import get_close_matches

import requests
from bs4 import BeautifulSoup

from flask import Flask, request, Response, jsonify, stream_with_context
from flask_cors import CORS

# –õ–æ–∫–∞–ª—å–Ω—ã–µ —É—Ç–∏–ª–∏—Ç—ã
from helpers import (
    normalize_text,
    tokenize,
    expand_keywords,
    build_law_index,
)

# ----------------------- –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ -----------------------
logging.basicConfig(
    level=os.getenv("LOG_LEVEL", "INFO"),
    format="%(asctime)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger("kaz-legal-bot")

# ----------------------- Flask & CORS ----------------------
app = Flask(__name__)

def _parse_origins(raw: str) -> List[str]:
    if not raw:
        return ["*"]
    parts = [p.strip() for p in raw.split(",") if p.strip()]
    return parts or ["*"]

CORS_ORIGINS = _parse_origins(os.getenv("CORS_ORIGINS", ""))
CORS(app, resources={r"/*": {"origins": CORS_ORIGINS}}, supports_credentials=True)
logger.info("‚úÖ CORS configured for origins: %s", CORS_ORIGINS)

# ----------------------- –ó–∞–∫–æ–Ω—ã / –ò–Ω–¥–µ–∫—Å -------------------
LAWS_PATH = os.getenv("LAWS_PATH", os.path.join("backend", "laws", "kazakh_laws.json"))

LAW_DB: List[Dict[str, Any]] = []
LAW_INDEX: Dict[str, set] = {}  # word -> set(article_idx)

def load_laws() -> None:
    global LAW_DB
    if not os.path.isfile(LAWS_PATH):
        raise FileNotFoundError(f"–ù–µ –Ω–∞–π–¥–µ–Ω —Ñ–∞–π–ª –∑–∞–∫–æ–Ω–æ–≤: {LAWS_PATH}")
    with open(LAWS_PATH, "r", encoding="utf-8") as f:
        data = json.load(f)

    if isinstance(data, dict) and "laws" in data:
        LAW_DB = data["laws"]
    elif isinstance(data, list):
        LAW_DB = data
    else:
        raise ValueError("–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞ –∑–∞–∫–æ–Ω–æ–≤.")

    for it in LAW_DB:
        it.setdefault("title", "")
        it.setdefault("text", "")
        it.setdefault("source", "")

    logger.info("‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ %d —Å—Ç–∞—Ç–µ–π –∏–∑ –±–∞–∑—ã –∑–∞–∫–æ–Ω–æ–≤.", len(LAW_DB))

def build_index() -> None:
    global LAW_INDEX
    LAW_INDEX = build_law_index(LAW_DB)
    logger.info("‚úÖ –ò–Ω–¥–µ–∫—Å –∑–∞–∫–æ–Ω–æ–≤ —É—Å–ø–µ—à–Ω–æ –ø–æ—Å—Ç—Ä–æ–µ–Ω. –í—Å–µ–≥–æ –∫–ª—é—á–µ–π: %d", len(LAW_INDEX))

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
load_laws()
build_index()

# ----------------------- –ü–æ–∏—Å–∫ –ø–æ –ª–æ–∫–∞–ª—å–Ω–æ–π –ë–î -------------
def correct_keyword(word: str, keys: Iterable[str], cutoff: float = 0.78) -> str:
    matches = get_close_matches(word, list(keys), n=1, cutoff=cutoff)
    return matches[0] if matches else word

def find_local_candidates(query: str, top_k: int = 6) -> List[int]:
    if not query.strip():
        return []

    expanded = expand_keywords(query)
    logger.info("üîé –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: %s", list(expanded)[:12])

    scores: Dict[int, float] = {}
    index_keys = LAW_INDEX.keys()

    for raw in expanded:
        key = raw
        if key not in LAW_INDEX:
            key = correct_keyword(key, index_keys)
        if key not in LAW_INDEX:
            continue

        for idx in LAW_INDEX[key]:
            scores[idx] = scores.get(idx, 0.0) + 1.0
            title = LAW_DB[idx].get("title", "")
            if key in normalize_text(title):
                scores[idx] += 1.2

    if not scores:
        return []

    ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)
    return [idx for idx, _ in ranked[:top_k]]

# ----------------------- –í–Ω–µ—à–Ω–∏–π –∏—Å—Ç–æ—á–Ω–∏–∫ (Adilet) ---------
ADILET_BASE = "https://adilet.zan.kz"
ADILET_SEARCH = f"{ADILET_BASE}/rus/search/docs?query="

def search_adilet_titles(query: str, max_results: int = 5, timeout: int = 8) -> List[Dict[str, str]]:
    results: List[Dict[str, str]] = []
    try:
        url = ADILET_SEARCH + requests.utils.quote(query)
        headers = {"User-Agent": "KazLegalBot/1.0"}
        r = requests.get(url, headers=headers, timeout=timeout)
        r.raise_for_status()
        soup = BeautifulSoup(r.text, "html.parser")

        for a in soup.find_all("a", href=True):
            href = a["href"]
            if "/rus/docs/" in href:
                title = re.sub(r"\s+", " ", a.get_text(strip=True))
                if not title:
                    continue
                full_url = href if href.startswith("http") else ADILET_BASE + href
                results.append({"title": title, "source": full_url})
                if len(results) >= max_results:
                    break
    except Exception as e:
        logger.warning("‚ö†Ô∏è Adilet –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: %s", e)

    return results

# ----------------------- –°–∏—Å—Ç–µ–º–Ω–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è --------------
def build_system_instruction(law_titles: List[Dict[str, str]]) -> str:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ñ–Å–°–¢–ö–ò–ï –ø—Ä–∞–≤–∏–ª–∞ –¥–ª—è –º–æ–¥–µ–ª–∏ + —Å–ø–∏—Å–æ–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏–π –∑–∞–∫–æ–Ω–æ–≤.
    –í–ê–ñ–ù–û: –ø—Ä–æ—Å–∏–º –≤—ã–≤–æ–¥–∏—Ç—å –¢–û–õ–¨–ö–û HTML.
    """
    lines = []
    lines.append(
        "SYSTEM:\n"
        "–¢—ã ‚Äî –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π –ò–ò-—é—Ä–∏—Å—Ç –ø–æ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤—É –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ –†–µ—Å–ø—É–±–ª–∏–∫–∏ –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω.\n"
        "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª—è–π —Å–µ–±—è –∫–∞–∫ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–≥–æ —é—Ä–∏—Å—Ç–∞, –≤—Å–µ–≥–¥–∞ –ø—ã—Ç–∞–π—Å—è –ø–æ–º–æ—á—å –∫–ª–∏–µ–Ω—Ç—É. –ù–∏–∫–æ–≥–¥–∞ –Ω–µ –≥–æ–≤–æ—Ä–∏ —á—Ç–æ –Ω–µ –º–æ–∂–µ—à—å –æ—Ç–≤–µ—Ç–∏—Ç—å.\n"
        "–ü—Ä–∞–≤–∏–ª–∞ –æ—Ç–≤–µ—Ç–∞:\n"
        "1) –§–æ—Ä–º–∞—Ç –≤—ã–≤–æ–¥–∞ ‚Äî –¢–û–õ–¨–ö–û HTML. –ù–∏–∫–∞–∫–æ–≥–æ Markdown –∏ **–∑–≤—ë–∑–¥–æ—á–µ–∫**. "
        "–ò—Å–ø–æ–ª—å–∑—É–π <p>, <ul>, <li>, <strong>, <em>, <h3>, <br>.\n"
        "2) –°–Ω–∞—á–∞–ª–∞ –¥–∞–π –∫—Ä–∞—Ç–∫—É—é —é—Ä–∏–¥–∏—á–µ—Å–∫—É—é –æ—Ü–µ–Ω–∫—É (—á—Ç–æ –ø—Ä–∏–º–µ–Ω–∏–º–æ, –Ω–∞—Ä—É—à–µ–Ω–æ –ª–∏ –ø—Ä–∞–≤–æ, –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å).\n"
        "3) –ó–∞—Ç–µ–º ‚Äî —á—ë—Ç–∫–∏–µ –ø–æ—à–∞–≥–æ–≤—ã–µ –¥–µ–π—Å—Ç–≤–∏—è (–∫—É–¥–∞ –∏–¥—Ç–∏/–ø–∏—Å–∞—Ç—å, –∫–∞–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã, —Å—Ä–æ–∫–∏/—Ä–∏—Å–∫–∏).\n"
        "4) –°—Å—ã–ª–∞–π—Å—è –Ω–∞ –ù–ü–ê –†–ö –ø–æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏, –ù–û –Ω–µ –≤—Å—Ç–∞–≤–ª—è–π –¥–ª–∏–Ω–Ω—ã—Ö —Ü–∏—Ç–∞—Ç; –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –Ω–∞–∑–≤–∞–Ω–∏—è —Å—Ç–∞—Ç—å–∏/–∫–æ–¥–µ–∫—Å–∞.\n"
        "5) –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –º–∞–ª–æ ‚Äî —Å–Ω–∞—á–∞–ª–∞ –¥–∞–π —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –¥–µ–π—Å—Ç–≤–∏–π, –∑–∞—Ç–µ–º –∑–∞–¥–∞–≤–∞–π —É—Ç–æ—á–Ω—è—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã —Å–ø–∏—Å–∫–æ–º.\n"
        "6) –ù–µ –æ—Ç–ø—Ä–∞–≤–ª—è–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è ¬´–∫ –¥—Ä—É–≥–æ–º—É —é—Ä–∏—Å—Ç—É¬ª; –¥–∞–π –º–∞–∫—Å–∏–º—É–º –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö —à–∞–≥–æ–≤ –∑–¥–µ—Å—å.\n"
        "7) –ï—Å–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ –ø–æ–∫—Ä—ã–≤–∞–µ—Ç –≤–æ–ø—Ä–æ—Å, –æ–±–æ–±—â–∞–π –±–µ–∑–æ–ø–∞—Å–Ω–æ –∏ —è–≤–Ω–æ —É–∫–∞–∂–∏, —á—Ç–æ –ø—Ä–∏–≤–µ–¥–µ–Ω—ã –æ–±—â–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏.\n"
        "8) –í –∫–æ–Ω—Ü–µ –¥–æ–±–∞–≤—å —Ä–∞–∑–¥–µ–ª <h3>–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∑–∞–∫–æ–Ω—ã</h3> —Å–æ —Å–ø–∏—Å–∫–æ–º –Ω–∞–∑–≤–∞–Ω–∏–π (–±–µ–∑ –±–æ–ª—å—à–∏—Ö —Ü–∏—Ç–∞—Ç).\n"
    )

    if law_titles:
        lines.append("–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∑–∞–∫–æ–Ω—ã (–Ω–∞–∑–≤–∞–Ω–∏—è –¥–ª—è –æ—Ä–∏–µ–Ω—Ç–∏—Ä–∞, –±–µ–∑ —Ü–∏—Ç–∞—Ç):")
        for i, l in enumerate(law_titles, 1):
            ttl = (l.get("title") or "").strip()
            src = (l.get("source") or "").strip()
            if ttl:
                if src:
                    lines.append(f"{i}. {ttl} ‚Äî {src}")
                else:
                    lines.append(f"{i}. {ttl}")

    # –ñ—ë—Å—Ç–∫–æ –∑–∞—è–∫–æ—Ä–∏–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ:
    lines.append(
        "\n–í—Å–µ–≥–¥–∞ –≤–æ–∑–≤—Ä–∞—â–∞–π –≤–∞–ª–∏–¥–Ω—ã–π HTML. –ü—Ä–∏–º–µ—Ä –º–∏–Ω–∏-—à–∞–±–ª–æ–Ω–∞:\n"
        "<h3>–ö—Ä–∞—Ç–∫–∏–π –≤—ã–≤–æ–¥</h3>\n"
        "<p>‚Ä¶</p>\n"
        "<h3>–ß—Ç–æ –¥–µ–ª–∞—Ç—å</h3>\n"
        "<ul><li>–®–∞–≥ 1‚Ä¶</li><li>–®–∞–≥ 2‚Ä¶</li></ul>\n"
        "<h3>–£—Ç–æ—á–Ω–∏—Ç–µ</h3>\n"
        "<ul><li>–í–æ–ø—Ä–æ—Å 1‚Ä¶</li></ul>\n"
        "<h3>–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∑–∞–∫–æ–Ω—ã</h3>\n"
        "<ul><li>–ù–∞–∑–≤–∞–Ω–∏–µ –∞–∫—Ç–∞/—Å—Ç–∞—Ç—å–∏‚Ä¶</li></ul>\n"
    )
    return "\n".join(lines)

# ----------------------- –ú–æ–¥–µ–ª—å / –§–æ–ª–±—ç–∫ -------------------
USE_MODEL = os.getenv("GEMINI_MODEL", "gemini-1.5-flash")

def generate_with_gemini(prompt: str) -> str:
    import google.generativeai as genai
    api_key = os.getenv("GEMINI_API_KEY", "").strip()
    if not api_key:
        raise RuntimeError("GEMINI_API_KEY is empty")

    genai.configure(api_key=api_key)
    model = genai.GenerativeModel(USE_MODEL)
    resp = model.generate_content(prompt)
    if not resp or not resp.text:
        raise RuntimeError("Empty model response")
    return resp.text

def ensure_html(s: str) -> str:
    """
    –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –≤–¥—Ä—É–≥ –≤–µ—Ä–Ω—É–ª–∞ –Ω–µ-HTML/markdown ‚Äì –∞–∫–∫—É—Ä–∞—Ç–Ω–æ –æ–±–æ—Ä–∞—á–∏–≤–∞–µ–º –≤ <p>.
    """
    if not s:
        return "<p>–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö.</p>"
    if re.search(r"</?[a-z][\s\S]*>", s, flags=re.I):
        return s
    # –ø—Ä–æ—Å—Ç–∞—è –∑–∞–º–µ–Ω–∞ –ø–µ—Ä–µ–≤–æ–¥–æ–≤ —Å—Ç—Ä–æ–∫ –Ω–∞ –∞–±–∑–∞—Ü—ã
    parts = [p.strip() for p in re.split(r"\n{2,}", s) if p.strip()]
    if not parts:
        return "<p>–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö.</p>"
    return "".join(f"<p>{p.replace('\n','<br>')}</p>" for p in parts)

def fallback_answer(question: str, laws: List[Dict[str, str]]) -> str:
    items = []
    items.append("<h3>–ö—Ä–∞—Ç–∫–∏–π –≤—ã–≤–æ–¥</h3>")
    items.append("<p>–Ø –ø–æ–¥–≥–æ—Ç–æ–≤–∏–ª –æ–±—â–∏–π –ø–ª–∞–Ω –¥–µ–π—Å—Ç–≤–∏–π –ø–æ –≤–∞—à–µ–º—É –≤–æ–ø—Ä–æ—Å—É. –ù–∏–∂—ã–µ ‚Äî –±–µ–∑–æ–ø–∞—Å–Ω—ã–µ —à–∞–≥–∏ –±–µ–∑ —Å—Å—ã–ª–æ–∫ –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –Ω–æ—Ä–º—ã.</p>")
    items.append("<h3>–ß—Ç–æ –¥–µ–ª–∞—Ç—å</h3>")
    items.append("<ul>")
    items.append("<li>–ö—Ä–∞—Ç–∫–æ –æ–ø–∏—à–∏—Ç–µ —Å–∏—Ç—É–∞—Ü–∏—é (–¥–∞—Ç—ã, —Å—Ç–æ—Ä–æ–Ω—ã, –¥–æ–∫—É–º–µ–Ω—Ç—ã).</li>")
    items.append("<li>–û–ø—Ä–µ–¥–µ–ª–∏—Ç–µ —Ü–µ–ª—å (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø—Ä–µ–∫—Ä–∞—Ç–∏—Ç—å —Ç—Ä—É–¥–æ–≤–æ–π –¥–æ–≥–æ–≤–æ—Ä –±–µ–∑ –æ—Ç—Ä–∞–±–æ—Ç–∫–∏ / –ø–æ–ª—É—á–∏—Ç—å —Ä–∞—Å—á—ë—Ç).</li>")
    items.append("<li>–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–æ–≥–æ–≤–æ—Ä –∏ –ª–æ–∫–∞–ª—å–Ω—ã–µ –∞–∫—Ç—ã —Ä–∞–±–æ—Ç–æ–¥–∞—Ç–µ–ª—è –Ω–∞ —Å–ø–µ—Ü—É—Å–ª–æ–≤–∏—è.</li>")
    items.append("<li>–ü–æ–¥–∞–π—Ç–µ –ø–∏—Å—å–º–µ–Ω–Ω–æ–µ –∑–∞—è–≤–ª–µ–Ω–∏–µ/–ø—Ä–µ—Ç–µ–Ω–∑–∏—é, —Å–æ–±–ª—é–¥–∞—è —Å—Ä–æ–∫–∏ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è.</li>")
    items.append("<li>–ü—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ ‚Äî –æ–±—Ä–∞—â–µ–Ω–∏–µ –≤ —Ç—Ä—É–¥–æ–≤—É—é –∏–Ω—Å–ø–µ–∫—Ü–∏—é –∏–ª–∏ —Å—É–¥.</li>")
    items.append("</ul>")
    items.append("<h3>–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∑–∞–∫–æ–Ω—ã</h3>")
    if laws:
        items.append("<ul>")
        for l in laws:
            ttl = (l.get("title") or "").strip()
            src = (l.get("source") or "").strip()
            if ttl:
                if src:
                    items.append(f"<li>{ttl} ‚Äî <a href=\"{src}\">{src}</a></li>")
                else:
                    items.append(f"<li>{ttl}</li>")
        items.append("</ul>")
    else:
        items.append("<p>–ü–æ–¥—Ö–æ–¥—è—â–∏—Ö —Å—Ç–∞—Ç–µ–π –≤ –ª–æ–∫–∞–ª—å–Ω–æ–π –±–∞–∑–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ. –£—Ç–æ—á–Ω–∏—Ç–µ –¥–µ—Ç–∞–ª–∏ –∑–∞–ø—Ä–æ—Å–∞.</p>")
    return "".join(items)

def stream_text_chunks(text: str, chunk_size: int = 1200) -> Iterable[str]:
    text = text.strip()
    for i in range(0, len(text), chunk_size):
        yield text[i:i + chunk_size]
        time.sleep(0.01)

# ----------------------- HTTP Handlers ---------------------
@app.route("/health", methods=["GET"])
def health():
    return jsonify({"status": "ok", "laws": len(LAW_DB), "index_keys": len(LAW_INDEX)})

@app.route("/chat", methods=["POST"])
def chat():
    try:
        payload = request.get_json(force=True, silent=False) or {}
        question = (payload.get("question") or payload.get("message") or "").strip()
        if not question:
            return jsonify({"error": "Empty question"}), 400

        logger.info("üó®Ô∏è –í–æ–ø—Ä–æ—Å: %s", question)

        # 1) –õ–æ–∫–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫
        local_ids = find_local_candidates(question, top_k=6)

        # 2) –í–Ω–µ—à–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –ø—Ä–∏ –Ω–µ—Ö–≤–∞—Ç–∫–µ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        external = []
        if not local_ids:
            external = search_adilet_titles(question, max_results=5)
        elif len(local_ids) < 3:
            external = search_adilet_titles(question, max_results=3)

        # 3) –ì–æ—Ç–æ–≤–∏–º —Å–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π (title + source)
        context_titles: List[Dict[str, str]] = []
        for idx in local_ids:
            law = LAW_DB[idx]
            context_titles.append({"title": law.get("title", ""), "source": law.get("source", "")})
        # –¥–æ–±–∞–≤–∏–º –≤–Ω–µ—à–Ω–∏–µ, –∏–∑–±–µ–≥–∞—è –¥—É–±–ª–µ–π –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é
        for e in external:
            if e.get("title") and not any(c["title"] == e["title"] for c in context_titles):
                context_titles.append({"title": e["title"], "source": e.get("source", "")})
        context_titles = context_titles[:6]

        # 4) –°–∏—Å—Ç–µ–º–Ω–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è + –≤–æ–ø—Ä–æ—Å
        system_instruction = build_system_instruction(context_titles)
        prompt = (
            f"{system_instruction}\n\n"
            f"USER QUESTION (–æ—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û HTML):\n{question}\n"
        )

        # 5) –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
        try:
            raw = generate_with_gemini(prompt)
            text = ensure_html(raw)
        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: %s", e)
            text = fallback_answer(question, context_titles)

        # 6) –°—Ç—Ä–∏–º–∏–Ω–≥ HTML
        def _gen():
            for chunk in stream_text_chunks(text):
                yield chunk

        return Response(stream_with_context(_gen()), mimetype="text/html; charset=utf-8")

    except Exception as e:
        logger.exception("–û—à–∏–±–∫–∞ /chat: %s", e)
        return jsonify({"error": str(e)}), 500


if __name__ == "__main__":
    port = int(os.getenv("PORT", "5000"))
    app.run(host="0.0.0.0", port=port, debug=False)
